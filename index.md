---
layout: default
---

I am an [IFML](https://www.ifml.institute) Postdoctoral Fellow based at [UT Austin](https://www.cs.utexas.edu) 
hosted by [Adam Klivans][klivans] and [Raghu Meka][meka].  I received my PhD in Computer Science from the University of
[Wisconsin-Madison][uwm], where I was advised by [Christos Tzamos][tzamos-page].  Prior to UW-Madison, I studied Electrical and Computer Engineering at the [National Technical University of Athens][ece-ntua] where I was advised by [Dimitris Fotakis][fotakis-page].  

I work on designing efficient algorithms with provable guarantees for machine learning problems 
with a focus on dealing with imperfect data (e.g., classification with noisy labels and 
statistical inference from biased or censored data).  I am also interested in analyzing and providing formal guarantees
for popular machine learning algorithms (e.g., diffusion models).

<b> I am on the 2024/25 job market. Here is my [CV](assets/cv/cv.pdf). </b>

## News 

* Aug, 22, 2024: I am visiting [Simons Institute](https://simons.berkeley.edu/homepage) at Berkeley for the [generalization](https://simons.berkeley.edu/workshops/emerging-generalization-settings) and [LLM](https://simons.berkeley.edu/programs/special-year-large-language-models-transformers-part-1) programs.

* Jul, 3, 2024: Our work [Smoothed Analysis for Learning Concepts of Low Intrinsic Dimension](https://arxiv.org/abs/2407.00966)
got the best paper award at [COLT 2024](https://learningtheory.org/colt2024/awards.html)!!

* May, 1, 2024: [New paper](https://arxiv.org/abs/2404.18893) on learning mixtures of Gaussians using diffusion models!




[uwm]: https://www.wisc.edu/
[ece-ntua]: https://www.ece.ntua.gr/en
[email-me]: mailto:vkonton@gmail.com
[tzamos-page]: https://tzamos.com/
[klivans]: https://www.cs.utexas.edu/users/klivans/
[meka]: https://raghumeka.github.io
[fotakis-page]: https://www.softlab.ntua.gr/~fotakis/
[thesis-link]: http://artemis.cslab.ntua.gr/Dienst/UI/1.0/Display/artemis.ntua.ece/DT2017-0274?abstract=%EA%EF%ED%F4%EF%ED%DE%F2
[sliu]: https://lteins.github.io
[ktian]: https://kjtian.github.io
[vasilyan]: https://www.vasilyan.net

## Publications

1. [Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension](https://arxiv.org/abs/2407.00966) <br/>
   w/ G. Chandrasekaran, [A. Klivans][klivans], [R. Meka][meka], K. Stavropoulos <br/>
   <b style='color:red;'> Best Paper Award </b> [COLT 2024](http://learningtheory.org/colt2024/) <br/>
   [IPAM 2024 Long Talk Video](https://www.youtube.com/watch?v=XJSz3XoNdlY)

1. [Active Learning with Simple Questions](https://arxiv.org/abs/2405.07937) <br/>
   w/ M. Ma, [C. Tzamos][tzamos] <br/>
   [COLT 2024](http://learningtheory.org/colt2024/)

1. [Agnostically Learning Multi-index Models with Queries](https://arxiv.org/abs/2312.16616) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [D. Kane][kane], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/> 
    [FOCS 2024](https://focs.computer.org/2024/)

1. [Super Non-singular Decompositions of Polynomials and their](https://arxiv.org/html/2404.00529v1) <br/>
   [Application to Robustly Learning Low-degree PTFs](https://arxiv.org/html/2404.00529v1) <br/>
   w/ [I. Diakonikolas][idiakonikolas], [D M. Kane][kane], [S. Liu][sliu], [N. Zarifis][zarifis] <br/>
   [STOC 2024](http://acm-stoc.org/stoc2024/)

1. [Efficient Discrepancy Testing for Learning with Distribution Shift](https://arxiv.org/abs/2406.09373)
    w/ [G. Chandrasekaran], [A. Klivans][klivans], K. Stavropoulos, [A. Vasilyan][vasilyan] <br/> 
    [NeurIPS 2024](https://nips.cc)

1. [Active Classification with Few Queries under Misspecification]
    w/ [C. Tzamos][tzamos], M. Ma <br/> 
    [NeurIPS 2024](https://nips.cc)

1. Learning Noisy Halfspaces with a Margin: Massart is no Harder than Random
    w/ [G. Chandrasekaran], K. Stavropoulos, [K. Tian][ktian] <br/> 
    [NeurIPS 2024](https://nips.cc)

1. [SLaM: Student-Label Mixing for Distillation with Unlabeled Examples](https://arxiv.org/abs/2302.03806) <br/>
    w/ [G. Chandrasekaran], K. Stavropoulos, [K. Tian][ktian] <br/> 
    [NeurIPS 2024](https://nips.cc)


1. [Optimizing Solution-Samplers for Combinatorial Problems:](https://arxiv.org/pdf/2310.05309.pdf) <br/>
   [The Landscape of Policy Gradient Methods](https://arxiv.org/pdf/2310.05309.pdf) <br/>
   w/ [C. Caramanis][caramanis], [D. Fotakis][fotakis], [A. Kalavasis][kalavasis], [C. Tzamos][tzamos] <br/>
   <b> Selected for Oral Presentation </b> <br/>
   [NeurIPS 2023](https://nips.cc) 

1. [SLaM: Student-Label Mixing for Distillation with Unlabeled Examples](https://arxiv.org/abs/2302.03806) <br/>
    w/ [F. Iliopoulos][iliopoulos], [K. Trinh][trinh], [C. Baykal][baykal], [G. Menghani][menghani],  [V. Erik][vee] <br/> 
    [NeurIPS 2023](https://nips.cc)

1. [The Gain from Ordering in Online Learning](https://openreview.net/pdf?id=OaUT4hX40s) <br/>
   w/ M. Ma, [C. Tzamos][tzamos] <br/>
   [NeurIPS 2023](https://nips.cc) 

1. [Efficient Testable Learning of Halfspaces with Adversarial Label Noise](https://arxiv.org/abs/2303.05485) <br/>
   w/ [I. Diakonikolas][idiakonikolas], [D M. Kane][kane], [S. Liu][sliu], [N. Zarifis][zarifis] <br/>
   [NeurIPS 2023](https://nips.cc) 

1. [Self Directed Linear Classification](https://arxiv.org/abs/2308.03142) <br/> 
   w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
   [COLT 2023](http://learningtheory.org/colt2023/)

1. [Weighted Distillation with Unlabeled Examples](https://arxiv.org/abs/2210.06711) <br/>
    w/ [F. Iliopoulos][iliopoulos], [C. Baykal][baykal], [G. Menghani][menghani], [K. Trinh][trinh], [V. Erik][vee] <br/> 
    [NeurIPS 2022](https://nips.cc)

1.  [Linear Label Ranking with Bounded Noise](https://openreview.net/pdf?id=dgWo-UyVEsa) <br/>
    w/ [D. Fotakis][fotakis], [A. Kalavasis][kalavasis], [C. Tzamos][tzamos] <br/>
    <b> Selected for Oral Presentation </b> <br/>
    [NeurIPS 2022](https://nips.cc)

1. [Learning General Halfspaces with Adversarial Label Noise via Online Gradient Descent](https://proceedings.mlr.press/v162/diakonikolas22b.html) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/> 
    [ICML 2022](https://icml.cc)

1. [Learning a Single Neuron with Adversarial Label Noise via Gradient Descent](https://arxiv.org/abs/2206.08918) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/> 
    [COLT 2022](http://learningtheory.org/colt2022/)

1. [Learning General Halfspaces with General Massart Noise under the Gaussian Distribution](https://arxiv.org/abs/2108.08767) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [D. Kane][kane], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/> 
    [STOC 2022](http://acm-stoc.org/stoc2022/)

1. [A Statistical Taylor Theorem and Extrapolation of Truncated Densities](https://arxiv.org/abs/2106.15908) <br/>
    w/ [C. Daskalakis][daskalakis], [C. Tzamos][tzamos], [M. Zampetakis][zampetakis] <br/>
    [COLT 2021](http://www.learningtheory.org/colt2021/)

1. [Agnostic Proper Learning of Halfspaces under Gaussian Marginals](https://arxiv.org/abs/2102.05629) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [D. Kane][kane], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
    [COLT 2021](http://www.learningtheory.org/colt2021/)

1. [Efficient Algorithms for Learning from Coarse Labels](http://proceedings.mlr.press/v134/fotakis21a.html) <br/>
    w/ [D. Fotakis][fotakis], [A. Kalavasis][kalavasis], [C. Tzamos][tzamos] <br/>
    [COLT 2021](http://www.learningtheory.org/colt2021/)

1. [Learning Online Algorithms with Distributional Advice](http://proceedings.mlr.press/v139/diakonikolas21a.html) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [A. Vakilian][vakilian], [N. Zarifis][zarifis] <br/>
    [ICML 2021](https://icml.cc)

1. [A Polynomial Time Algorithm For Learning Halfspaces with Tsybakov Noise](https://arxiv.org/abs/2010.01705) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [D. Kane][kane], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
    [STOC 2021](http://acm-stoc.org/stoc2021/)

1. [Learning Halfspaces with Tsybakov Noise](https://arxiv.org/abs/2006.06467) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
    [STOC 2021](http://acm-stoc.org/stoc2021/) <br/>
    Conference version merged with the above paper

1. [Non-Convex SGD Learns Halfspaces with Adversarial Label Noise](https://arxiv.org/abs/2006.06742) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
    [NeurIPS 2020](http://learningtheory.org/colt2020/)

1.  [Learning Halfspaces with Massart Noise Under Structured Distributions](https://arxiv.org/abs/2002.05632)  <br/>
    w/ [I. Diakonikolas][idiakonikolas], [C. Tzamos][tzamos], [N. Zarifis][zarifis] <br/>
    [COLT 2020](http://learningtheory.org/colt2020/)

1.  [Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer ReLU Networks](https://arxiv.org/abs/2006.12476) <br/>
    w/ [I. Diakonikolas][idiakonikolas], [D. Kane][kane], [N. Zarifis][zarifis] <br/>
    [COLT 2020](http://learningtheory.org/colt2020/)

1.  [Efficient Truncated Statistics with Unknown Truncation](https://arxiv.org/abs/1908.01034) <br/>
    w/ [C. Tzamos][tzamos], [M. Zampetakis][zampetakis] <br/>
    [FOCS 2019](http://focs2019.cs.jhu.edu/)

1.  Removing Bias in Maching Learning via Truncated Statistics <br/>
    w/ [C. Daskalakis][daskalakis],  [C. Tzamos][tzamos], [M. Zampetakis][zampetakis] <br/>
    Manuscript

1. [Opinion Dynamics with Limited Information][OpinionDynamics] <br/>
   w/ [D. Fotakis][fotakis], V. Kandiros,  [S. Skoulakis][skoulakis] <br/>
   [WINE 2018](https://www.cs.ox.ac.uk/conferences/wine2018/)

1. [Learning Powers of Poisson Binomial Distributions][PBDpowers] <br/>
   w/ [D. Fotakis][fotakis], [P. Krysta][krysta], [P. Spirakis][spirakis] <br/>
   Manuscript


[OpinionDynamics]:https://github.com/vkonton/opinion_dynamics/blob/master/clean.pdf
[PBDpowers]:http://arxiv.org/abs/1707.05662
[zarifis]:https://nikoszarifis.github.io
[caramanis]:https://caramanis.github.io
[fotakis]:https://www.softlab.ntua.gr/~fotakis/
[krysta]:http://cgi.csc.liv.ac.uk/~piotr/
[kane]:https://cseweb.ucsd.edu/~dakane/
[spirakis]:https://intranet.csc.liv.ac.uk/news/item.php?id=19
[tzamos]:https://tzamos.com
[zampetakis]:http://www.mit.edu/~mzampet/
[daskalakis]:https://people.csail.mit.edu/costis/
[skoulakis]:http://www.corelab.ntua.gr/~sskoul/
[idiakonikolas]:http://www.iliasdiakonikolas.org/
[vakilian]:http://www.mit.edu/~vakilian/
[kalavasis]:https://alkisk.github.io
[vee]:https://scholar.google.com/citations?user=1u8drP0AAAAJ&hl=en
[trinh]:https://scholar.google.com/citations?user=pVTeodYAAAAJ&hl=en
[baykal]:https://people.csail.mit.edu/baykal/
[menghani]:http://www.gaurav.ai
[iliopoulos]:https://filiop.org
[meka]:https://hackmd.io/@raghum/index
[klivans]:https://www.cs.utexas.edu/users/klivans/


## Service
 * **Program Committees:** [ITCS-2025](http://itcs-conf.org)

 * **Reviewer:** FOCS (2020, 2021, 2023) , STOC (2020, 2024), COLT (2023), SODA (2019), NeurIPS (2021, 2023), WINE(2018), ICML(2023, 2021, 2020), 
EC (2022, 2020), MFCS (2018), TCS (2018, 2021), ALT (2021)


## Talks

* Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension
  1. <b> Best Paper Award Talk, COLT 2024 </b> 
  2.  EnCORE Workshop on Computational vs Statistical Gaps in Learning and Optimization, 2024 UCLA [IPAM 2024](https://www.youtube.com/watch?v=XJSz3XoNdlY) 
  3. Theory Seminar, 2024, University of Southern California 

* Optimizing Solution-Samplers for Combinatorial Problems, [NeurIPS 2023 Oral](https://nips.cc/virtual/2023/oral/73826)

* SLaM: Student-Label Mixing for Distillation with Unlabeled Examples, [NeurIPS 2023](https://nips.cc/virtual/2023/poster/71876)

* Learning General Halfspaces with General Massart Noise, [STOC 2022](https://www.youtube.com/watch?v=jm8K7_7HjZE)

* A Statistical Taylor's Theorem and Extrapolation of Truncated Densities, COLT 2021

* Agnostic Proper Learning of Halfspaces under Gaussian Marginals COLT 2021

* Efficient Algorithms for Learning Halfspaces with Tsybakov Noise, STOC 2021

* Non-Convex SGD Learns Halfspaces with Adversarial Label Noise, NeurIPS 2020

* Learning Halfspaces with Massart Noise Under Structured Distributions, COLT 2020

* [Efficient Truncated Statistics with Unknown Truncation][truncated_unknown],
  FOCS 2019, [Video]


* [Learning PBD Powers][pbdpowers],
   ECCO Research Seminar 2017, University of Liverpool


* Learning Theory Study Group, Corelab NTUA, 2017
   1. [Intoduction, No Free Lunch, Bias-Variance Tradeoff][learning1]
   2. [VC-Dimension][learning2]


* [Convex Optimization Minicourse][convex-minicourse], Corelab NTUA, 2017
   1. [Convex Problems, LP][convex1]
   2. [QP, SOCP][convex2]
   3. [SDP, GW MaxCut][convex3]
   4. [Vector Optimization, Duality][convex4]


* [Programming with Dependent Types][dependent], NTUA, 2015

[truncated_unknown]: assets/talks/focs2019.pdf
[video]: https://www.youtube.com/watch?v=6crXE-ANK6Y&list=PL3DbynX8gwfLXOsziSLaVmiLKKjedlvks

[pbdpowers]: assets/talks/pbdPowers.pdf

[learning1]: assets/talks/learning1.pdf
[learning2]: assets/talks/learning2.pdf

[convex-minicourse]: assets/talks/convex.pdf
[convex1]: assets/talks/convex1.pdf
[convex2]: assets/talks/convex2.pdf
[convex3]: assets/talks/convex3.pdf
[convex4]: assets/talks/convex4.pdf

[dependent]: assets/talks/dependent.pdf
